{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional packages needed\n",
    " \n",
    "To run the code you may need additional packages.\n",
    "packages: pandas,random,sklearn and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional packages needed\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics as skm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will be using the [UCI Machine Learning Repository: Adult Data](https://archive.ics.uci.edu/ml/datasets/Adult) to predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'http://nikbearbrown.com/YouTube/MachineLearning/M09/adult.data.txt'\n",
    "adult_data = pd.read_csv(data_url,header=None)\n",
    "adult_data.columns = ['Age','Work Class','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capaital-gain','capital-loss','hours-per-week','native-country','Flag']\n",
    "#adult_data = adult_data.iloc[:, :-1]\n",
    "adult_data = pd.DataFrame(pd.get_dummies(adult_data))\n",
    "#adult_data.head(5)\n",
    "#print(adult_data.shape[0])\n",
    "train_row = random.sample(range(0,adult_data.shape[0]), (3 * adult_data.shape[0]) // 4)\n",
    "test_row = list(set(range(0,adult_data.shape[0]))-set(train_row))\n",
    "#print(len(train),len(test_row))\n",
    "train_data = adult_data.iloc[train_row,:]\n",
    "test_data = adult_data.iloc[test_row,:]\n",
    "test_data.head()\n",
    "X_train = train_data.iloc[:, :-2]\n",
    "#X_train.head()\n",
    "Y_train = train_data.iloc[:, -1]\n",
    "#print(Y_train)\n",
    "X_test = test_data.iloc[:, :-2]\n",
    "#X_test.head(5)\n",
    "Y_test = test_data.iloc[:, -1]\n",
    "#Y_test.head()\n",
    "#print(train_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Boosting\n",
    "\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In these sense it \"learns.\" Unlike Boosting, weights may change at the end of boosting round making certain learners more important than others. In some cases, boosting has been shown to yield better accuracy than Boosting, but it also tends tp propgate bias from the overweighting winning predictor and is more likely to over-fit the training data. By far, the most common implementation of Boosting is Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602137329566393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8624247635425624\n"
     ]
    }
   ],
   "source": [
    "control_term = DecisionTreeClassifier(max_depth=5) \n",
    "# Ada boosting algorithm\n",
    "adaBoosting_model = AdaBoostClassifier(n_estimators=20, base_estimator=control_term)\n",
    "adaBoosting_model.fit(X_train, Y_train)\n",
    "print(\"%s\" % adaBoosting_model.score(X_test,Y_test))\n",
    "#conf = skm.accuracy_score(Y_test,adaBoosting_model.predict(X_test), normalize=False)\n",
    "#conf1 = confusion_matrix(Y_test, adaBoosting_model.predict(X_test))\n",
    "#print(conf,conf1)\n",
    "\n",
    "#Ada XGBoost algorithm\n",
    "XGBoosting_model = xgb.XGBClassifier()\n",
    "XGBoosting_model.fit(X_train,Y_train)\n",
    "print(\"%s\" % XGBoosting_model.score(X_test,Y_test))\n",
    "#Matrix to check the predicted and actual value\n",
    "#y_original =pd.Series(Y_test,name='Actual')\n",
    "#y_predicated = pd.Series(XGBoosting_model.predict(X_test),name='Predicated')\n",
    "#df_confusion = pd.crosstab(y_original,y_predicated)\n",
    "#df_confusion"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "# Resources   \n",
    "\n",
    "* [An Attempt to Understand Boosting Algorithm(s) via @rbloggers](http://www.r-bloggers.com/an-attempt-to-understand-boosting-algorithms/)    \n",
    "\n",
    "* [Boosting](http://www.stat.cmu.edu/~ryantibs/datamining/lectures/25-boost.pdf)    \n",
    "\n",
    "* [boosting {adabag} | inside-R | A Community Site for R](http://www.inside-r.org/packages/cran/adabag/docs/boosting)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
